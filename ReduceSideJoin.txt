import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
 
// This class contains the MAppers and Reducer Classes.
 public class ReduceSideJoin {
 public static class CustomerMapper extends Mapper <LongWritable, Text, Text, Text>
 {
//This Map method is called for each <key,Value> pair in input split for CustomerDetails.csv
 public void map(LongWritable key, Text value, Context context)
 throws IOException, InterruptedException 
 {
 String line = value.toString();
 String[] records = line.split(",");
 context.write(new Text(records[0]), new Text("state\t" + records[4]));
 }
 }
 
 public static class SalesMapper extends Mapper <LongWritable, Text, Text, Text>
 {
    //This Map method is called for each <key,Value> pair in input split for CustomerTransaction.csv
 public void map(LongWritable key, Text value, Context context) 
 throws IOException, InterruptedException 
 {
 String line = value.toString();
 String[] records = line.split(",");
 context.write(new Text(records[1]), new Text("sales\t" + records[2]));
 }
 }
//This is the Reducer class which will aggregate the mapper class output 
 public static class CustSalesReducer extends Reducer <Text, Text, Text, FloatWritable>
 {
 public void reduce(Text key, Iterable<Text> values, Context context)
 throws IOException, InterruptedException 
 {
 String  state= "";
 float salesSum = 0;
 for (Text t : values) 
 { 
 String records[] = t.toString().split("\t");
 if (records[0].equals("sales")) 
 {
     salesSum += Float.parseFloat(records[1]);
 } 
 else if (records[0].equals("state")) 
 {
     state = records[1];
 }
 }
     context.write(new Text(state), new FloatWritable(salesSum));
 }
 }
 

//This is the driver code which configures the Map-Reduce job. 
 public static void main(String[] args) throws Exception {
 Configuration conf = new Configuration();
 @SuppressWarnings("deprecation")
Job job = new Job(conf, "Reduce-side join");
 job.setJarByClass(ReduceSideJoin.class);
 job.setReducerClass(CustSalesReducer.class);
 job.setMapOutputKeyClass(Text.class);
 job.setMapOutputValueClass(Text.class);
 job.setOutputKeyClass(Text.class);
 job.setOutputValueClass(FloatWritable.class);
 
 
 MultipleInputs.addInputPath(job, new Path(args[0]),TextInputFormat.class, CustomerMapper.class);
 MultipleInputs.addInputPath(job, new Path(args[1]),TextInputFormat.class, SalesMapper.class);
 Path outputPath = new Path(args[2]);
  
 FileOutputFormat.setOutputPath(job, outputPath);
 outputPath.getFileSystem(conf).delete(outputPath);
 conf.set("mapred.textoutputformat.separatorText", ",");
 System.exit(job.waitForCompletion(true) ? 0 : 1);
 }
 }